<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article article-type="book-review" dtd-version="2.3" xml:lang="EN" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">SCIENCE</journal-id>
<journal-title>Science</journal-title>
<abbrev-journal-title abbrev-type="pubmed">Science</abbrev-journal-title>
<issn pub-type="ppub">0036-8075</issn>
<issn pub-type="epub">1095-9203</issn>
<publisher>
<publisher-name>American Association for the Advancement of Science</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1237522</article-id>
<article-id pub-id-type="doi">10.1126/science.1237522</article-id>
<article-categories>
<subj-group subj-group-type="article-type"><subject>Book Review</subject></subj-group>
<subj-group subj-group-type="heading"><subject>Books <italic>et al.</italic></subject></subj-group>
<subj-group subj-group-type="legacy-article-type"><subject>books</subject></subj-group>
<subj-group subj-group-type="field"><subject>SCI POLICY</subject></subj-group>
<subj-group subj-group-type="display-ad"><subject>Medicine</subject><subject>Education</subject><subject>Economics</subject></subj-group>
<subj-group subj-group-type="overline"><subject>Social Science</subject></subj-group>
</article-categories>
<title-group>
<article-title>To Improve Public Policy</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes"><name><surname>Baird</surname><given-names>Brian</given-names></name></contrib>
<aff>The reviewer, a member of the National Academy of Sciences Division of Behavioral and Social Sciences and Education Advisory Committee, was the U.S. Representative for Washington's 3rd District from 1999 to 2011.</aff>
</contrib-group>
<author-notes>
<corresp>E-mail: <email>brianbairdphd@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="ppub">
<day>26</day>
<month>04</month>
<year>2013</year>
</pub-date>
<volume>340</volume>
<issue>6131</issue>
<fpage>432</fpage>
<lpage>433</lpage>
<product product-type="book"><bold>Evidence-Based Policy</bold> A Practical Guide to Doing It Better <bold><italic>by Nancy Cartwright and Jeremy Hardie</italic></bold> Oxford University Press, Oxford, 2012. 208 pp. $74.95, &#x00A3;45. ISBN 9780199841608. Paper, $18.95, &#x00A3;11.99. ISBN 9780199841622.</product>
<product product-type="book"><bold>Public Policy in an Uncertain World</bold> Analysis and Decisions <bold><italic>by Charles F. Manski</italic></bold> Harvard University Press, Cambridge, MA, 2013. 215 pp. $39.95, &#x00A3;29.95, &#x20AC;36. ISBN 9780674066892.</product>
<permissions>
<copyright-statement>Copyright &#x00A9; 2013, American Association for the Advancement of Science</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder>American Association for the Advancement of Science</copyright-holder>
</permissions>
<abstract abstract-type="teaser"><p>The authors explore the complexities of applying science in forming public policy and suggest means for making more effective use of research results.</p></abstract>
<abstract abstract-type="web-summary"><p>The authors explore the complexities of applying science in forming public policy and suggest means for making more effective use of research results.</p></abstract>
<custom-meta-wrap><custom-meta><meta-name>Editor</meta-name><meta-value>Sherman J. Suter</meta-value></custom-meta><custom-meta><meta-name>Copyeditor</meta-name><meta-value>Trista Wagoner</meta-value></custom-meta></custom-meta-wrap></article-meta>
</front>
<body>
<p>The impact of scientific discoveries has never been greater, but the ability of science to affect public policy, at least in the United States, is foundering. Which raises the question, how can science and scientists better inform public policy? Nancy Cartwright and Jeremy Hardie's <italic>Evidence-Based Policy: A Practical Guide to Doing It Better</italic> and Charles Manski's <italic>Public Policy in an Uncertain World: Analysis and Decisions</italic> attempt to answer this question. Consistent with their internal logic, they succeed and fall short to different degrees and for different reasons. Setting aside, for the moment, the much deeper, more troubling, and in these works largely unasked question of whether policy-makers and citizens actually want science to inform policy, it is worth examining each book from the perspectives of both academia and applied policy.</p>
<p>For policy-makers and students of policy, the more useful of the two books is likely to be that by philosophers Cartwright and Hardie (London School of Economics). The authors' particular concern is the elevation of and dependence on randomized controlled trials (RCTs) as the gold standard against which the quality of all research must be weighed and by which social policy and practice of everything from education to health care increasingly must be guided. The problem, as presented, is not with the internal validity of RCTs but with the way in which findings of a given RCT are applied to different situations by policy-makers with inadequate attention to all the factors that can profoundly influence the outcome in the new setting. The authors identify this challenge as how one gets from the evidence that something worked &#x201C;there&#x201D; (i.e., in the circumstances of a specific RCT or even multiple RCTs) to the conclusion that it will work &#x201C;here&#x201D; (i.e., in the new circumstances in which a policy-maker seeks to solve a problem that, at least on the surface, appears to be similar).</p>
<p>Cartwright and Hardie refer to the now fairly numerous evidence-based clearinghouses, such as the U.S. Department of Education's What Works Clearinghouse and the Campbell Collaboration. Even if the methodological conclusions of the clearinghouses are accepted as givens (which the authors are willing to grant), earnest policy-makers still face a difficult task. They must decide what, if anything, to do with that information when confronted with a real-world problem in a here, where a policy action is needed, that may be different from the there in which the original research was performed. And there, or rather here, lies the greatest value of the work.</p>
<p>The differences between internal and external validity and efficacy versus effectiveness are often discussed by scientists but too rarely appreciated by policy-makers and, according to the authors, understood too simplistically and largely impractically by researchers and policy-makers alike. After a lengthy, and (for a policy-maker) unnecessarily technical, discussion of the many factors that can differ between experimental and applied worlds, the authors offer a number of more practical suggestions and examples of how this translational decision process can be made.</p>
<fig id="F1" position="float">
<graphic xlink:href="340_432_F1"></graphic>
<attrib>CREDIT: RYAN JORGENSEN/FOTOSEARCH</attrib>
</fig>
<p>Even though fairly short, <italic>Evidence-Based Policy</italic> is redundant in parts, and the key points could be stated more simply and succinctly. Nevertheless, it would be worthwhile for policy-makers and well-intentioned researchers to consider carefully the caveats Cartwright and Hardie raise before recommending that a &#x201C;plug and play&#x201D; intervention &#x201C;proven&#x201D; to be effective in one setting be applied formulaically elsewhere. Failure to do so, as the authors point out, is likely to squander public resources without achieving the desired outcomes. Worse still, unintended and undesired consequences may result from well-intended and what appeared, superficially, to be evidence-based interventions.</p>
<p><italic>Public Policy in an Uncertain World</italic> focuses on a different but related science-policy shortcoming. Manski (an economist at Northwestern University) argues that scientists too often offer advice that conveys what he calls &#x201C;incredible certitude&#x201D;&#x2014;that gives an impression the scientific evidence is both more conclusive and more precise than is actually warranted. At the same time, Manski observes that scientific expressions of uncertainty and recognition of the counterfactual are too rare. He cautions, &#x201C;When researchers overreach, they not only give away their own credibility, but they diminish public trust in science more generally.&#x201D; As an antidote to incredible certitude, Manski advocates, among other recommendations, greater reporting of &#x201C;credible interval scoring&#x201D; versus single point estimates.</p>
<p>Like Cartwright and Hardie, Manski directs much of his critique to the overreliance on RCTs and the challenge of extrapolation from research findings to policy applications. To this, he adds commentary on the shortcomings of peer review and the inherent logical and practical limitations of various methodological and research design features ranging from subject selection to randomization to choice of statistical analysis and reporting.</p>
<p>To illustrate his points, Manski cites examples of the gaps between economic theory, research, and policy as well as social interventions, including evaluation of the effects of different criminal sentencing approaches. Much of his discussion centers on long-standing debates within economics and decision theory.</p>
<p>To academic readers steeped in these disciplines, his account is likely to be of some interest. It includes many useful and important insights (for example, the distinctions among policies based on the principles of &#x201C;maximin,&#x201D; &#x201C;minimax,&#x201D; and &#x201C;adaptive minimax&#x201D; regret) that have substantial implications for real-world policy. Unfortunately, from a practical perspective, those tend to get lost in more technical and polemic discussions of the writing.</p>
<p>Such criticism may seem like nitpicking, or even an irrelevance, to the academic community. However, if part of the stated goal of the work is to inform policy decisions, it is important that the informational process be applicable and accessible to the target audience.</p>
<p>Herein lies one of the shortcomings of both works and, indeed, of most related recent books on the subject. Just as it can be challenging to translate research findings from the there of one study to the here of a new application, the translation of theory and findings from the there of academia to the here of policy is more difficult than most academics or policy-makers appreciate. Speaking as a former member of Congress and a former academic researcher, I know this too well from both sides. Coming from that dual perspective, I found both books to be interesting, if sometimes unnecessarily challenging. Unfortunately, it seems likely that the odds are against their recommendations having the broader applied effects the authors seek.</p>
<p>Such a lack of impact would be a particular shame in two key areas: Manski provides thoughtful and very practical suggestions for how and why the U.S. Food and Drug Administration should alter its approach by implementing a process of &#x201C;adaptive partial drug approval.&#x201D; Cartwright and Hardie put forward analysis and models that should (in simpler form) be integrated into, and prominently featured as guidance in, all research warehouses&#x2014;to avoid purveying evidence-based policy recommendations without sufficient attention as to how they should, or should not, be applied.</p>
<p>Finally, any effort to improve how science informs policy must deal with the disturbing reality that a great many policy-makers in the United States today don't necessarily want science to inform policy to begin with. This, I am deeply sorry to say, is true even on the House of Representatives Science and Technology Committee. <italic>Evidence-Based Policy</italic> and <italic>Public Policy in an Uncertain World</italic> could contribute to the committee's understanding and deliberations. However, reading both books, I could, sadly, just as easily imagine key points being extracted or misapplied by certain committee members to justify the very unscientific processes and conclusions that now distort so much of how Congress functions. That is certainly not the fault of these well-intentioned authors, but it may well limit the applicability of their lessons.</p>
</body>
</article>